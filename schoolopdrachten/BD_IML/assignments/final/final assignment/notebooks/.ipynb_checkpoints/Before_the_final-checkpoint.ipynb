{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "from skimage import io, transform, color, filters, data, morphology, measure\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    images_name = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = io.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            images_name.append(filename[0:4])\n",
    "    \n",
    "    images_df = pd.DataFrame({\"name\": images_name,\n",
    "                             \"image\": images})\n",
    "    #print(images_df)\n",
    "    return images_df\n",
    "\n",
    "def display(np_image):\n",
    "    \"\"\"\n",
    "    This is a display function that we have added to show numpy images at full size\n",
    "    If you pass in an image with 3 channels, it will be displayed in RGB\n",
    "    If you passn in an image with 1 channel, it will be displayed in grayscale\n",
    "    \"\"\"\n",
    "    dpi = matplotlib.rcParams['figure.dpi']\n",
    "    if len(np_image.shape) == 3:\n",
    "        height, width, depth = np_image.shape\n",
    "    else:\n",
    "        height, width = np_image.shape\n",
    "\n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Display the image in either RGB or grayscale (depending on the amount of dimensions)\n",
    "    if (len(np_image.shape) >= 3):\n",
    "        ax.imshow(np_image)\n",
    "    else:\n",
    "        ax.imshow(np_image, cmap='gray')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "all_images_df = load_images_from_folder('../dataset-images/all_images')\n",
    "all_images_df\n",
    "train_images_df = all_images_df.sample(frac=0.9, random_state=25)\n",
    "#print('-------80%------')\n",
    "test_images_df = all_images_df.drop(train_images_df.index)\n",
    "\n",
    "train_images_df.reset_index(inplace=True,drop=True)\n",
    "test_images_df.reset_index(inplace=True,drop=True)\n",
    "#print(len(list_images))\n",
    "#print(list_images[0])\n",
    "#print(list_images_name)\n",
    "#print('----------done---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_image_grids(image, image_name):\n",
    "    zipcode = image\n",
    "    zipcode = zipcode *-1\n",
    "    gray = color.rgb2gray(zipcode)\n",
    "    thresh = filters.threshold_otsu(gray)\n",
    "    binary = gray > thresh\n",
    "    binary_splitted = [binary[:, :32], binary[:, 32:64], binary[:, 64:96], binary[:, 96:128]]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    lol = len(binary_splitted)\n",
    "    if (lol < 4):\n",
    "        print('splitted is smaller than 4')\n",
    "    for x in range(0, len(binary_splitted)):\n",
    "        \n",
    "        test = binary_splitted[x]\n",
    "        height = 8\n",
    "        width = 8\n",
    "\n",
    "        s_width = 0\n",
    "        s_heigth = 0\n",
    "        #print('lol: ', image_name[x])\n",
    "        df.loc[x, 'zip_number'] = image_name[x]\n",
    "\n",
    "        for i in range(1, 17):\n",
    "            h = s_heigth+height\n",
    "            w = s_width+width\n",
    "            feat = test[s_heigth:h, s_width:w]\n",
    "            white_pixels = feat[feat==1]\n",
    "            total_white_pixels = len(white_pixels)\n",
    "            df.loc[x, i] = total_white_pixels\n",
    "            if (i%4 == 0):\n",
    "                s_width = 0\n",
    "                s_heigth = s_heigth + height\n",
    "            else:\n",
    "                s_width = s_width + width\n",
    "            #plt.subplot(4,4,i)\n",
    "            #plt.imshow(feat)\n",
    "            #display(feat)\n",
    "\n",
    "        #pd.concat([df, df_tmp])\n",
    "        \n",
    "    return df\n",
    "\n",
    "image_properties = ['label', 'area', 'centroid', \n",
    "                    'perimeter', 'eccentricity', 'euler_number',\n",
    "                    'filled_area','perimeter_crofton', 'local_centroid', \n",
    "                    'major_axis_length', 'minor_axis_length', 'orientation']\n",
    "\n",
    "def initialize_image_props(image, image_name):\n",
    "    zipcode = image\n",
    "    zipcode = zipcode *-1\n",
    "    gray = color.rgb2gray(zipcode)\n",
    "    thresh = filters.threshold_otsu(gray)\n",
    "    binary = gray > thresh\n",
    "    #display(binary)\n",
    "    \n",
    "    # remove the white small spots/dots\n",
    "    binary = morphology.binary_opening(binary)\n",
    "    #display(binary)\n",
    "    \n",
    "    # make the white zip numbers thicker\n",
    "    binary = morphology.binary_dilation(binary)\n",
    "    #display(binary)\n",
    "    \n",
    "    # some numbers may have holes in it. That could seperate the number in 2 different labels. with closing the gap will be closed\n",
    "    binary = morphology.binary_closing(binary)\n",
    "    #display(binary)\n",
    "    \n",
    "    # return number back to its original state before it was made thicker. make the numbers thinner\n",
    "    binary = morphology.binary_erosion(binary)\n",
    "    #display(binary)\n",
    "    # return_num=True, connectivity=None\n",
    "    #print(binary)\n",
    "    label_image, total_labels = measure.label(binary, return_num=True)\n",
    "    #display(label_image)\n",
    "    global image_properties\n",
    "    props = measure.regionprops_table(label_image, properties=image_properties)\n",
    "    tmp_df = pd.DataFrame(props)\n",
    "    \n",
    "    # filter out labels that have too small areas. these labels are small dots that could not be filled with the opening function\n",
    "    tmp_df = tmp_df.query('area > 50').reset_index(drop=True)\n",
    "    #print(tmp_df)\n",
    "    zip_numbers = list(image_name)[:4]\n",
    "#    if len(tmp_df.index) == 4:\n",
    "#        #print('-----------------')\n",
    "#        #print(total_labels)\n",
    "#        #tmp_df.insert(0, 'zip_number', zip_numbers)\n",
    "#    else:\n",
    "#        print('indexed is not 4, but it is: ' + len(tmp_df.index))\n",
    "#        return None\n",
    "        \n",
    "    return tmp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(train_images_df.index)):\n",
    "    image_name = train_images_df.iloc[i,0]\n",
    "    image = train_images_df.iloc[i,1]\n",
    "    tmp_result_grid = initialize_image_grids(image, image_name)\n",
    "    tmp_result_props = initialize_image_props(image, image_name)\n",
    "    tmp_result = pd.concat([tmp_result_grid,tmp_result_props], axis=1)\n",
    "    result = pd.concat([result, tmp_result])\n",
    "\n",
    "\n",
    "result.reset_index(inplace=True, drop=True)\n",
    "result.drop(columns=['label'], inplace=True)\n",
    "#result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,\n",
    "            'major_axis_length', 'minor_axis_length', 'euler_number', \n",
    "            'centroid-0', 'eccentricity', 'local_centroid-0', 'perimeter', \n",
    "            'perimeter_crofton', 'filled_area', 'orientation']\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "minMaxScaler = MinMaxScaler()\n",
    "\n",
    "def preprocess(X_train, X_test):\n",
    "    print('-----Start preprocc----------')\n",
    "    global features\n",
    "    X_train[features] = minMaxScaler.fit_transform(X_train[features].to_numpy())\n",
    "    X_test[features] = minMaxScaler.transform(X_test[features].to_numpy())\n",
    "    print('---------Preprocess Done--------')\n",
    "    return X_train, X_test\n",
    "\n",
    "def splitTrainTest(result):\n",
    "    # Split data into 50% train and 50% test subsets\n",
    "    global features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        result[features], result['zip_number'], test_size=0.25, random_state=0)\n",
    "    \n",
    "    X_train, X_test = preprocess(X_train, X_test)\n",
    "    print('-----------dataset splitted------------')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "-----------dataset splitted------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = splitTrainTest(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_fit_score(X_train, X_test, y_train, y_test):\n",
    "    train_samples = len(X_train)\n",
    "    test_samples = len (X_test)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='chebyshev')\n",
    "    neigh.fit(X_train, y_train.to_numpy().reshape(train_samples))\n",
    "\n",
    "    score = neigh.score(X_test, y_test.to_numpy().reshape(test_samples))\n",
    "    return score\n",
    "\n",
    "def kfolding_knn(dataset):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    global features\n",
    "    X_training = dataset[features]\n",
    "    y_training = dataset['zip_number']\n",
    "    k = 1\n",
    "    for train_index, test_index in kf.split(X_training, y_training):\n",
    "        \n",
    "        X_train, X_test = X_training.loc[train_index,:], X_training.loc[test_index,:]\n",
    "        y_train, y_test = y_training.loc[train_index], y_training.loc[test_index]\n",
    "        X_train, X_test = preprocess(X_train, X_test)\n",
    "        \n",
    "        score = knn_fit_score(X_train, X_test, y_train, y_test)\n",
    "        print(\"[fold {0}], score: {1:.5f}\".\n",
    "          format(k, score))\n",
    "        k = k + 1\n",
    "        \n",
    "def searchHyperParams_knn(result):\n",
    "    print('--------start searching params--------')\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(result)\n",
    "    print('--------splitted train test----------')\n",
    "    grid_params = {\n",
    "        'n_neighbors' : [3,5,11,14,19],\n",
    "        'weights' : ['uniform', 'distance'],\n",
    "        'metric' : ['euclidean', 'manhattan', 'chebyshev']\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        KNeighborsClassifier(),\n",
    "        grid_params,\n",
    "        verbose = 1,\n",
    "        cv = 3,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    print('---------grid search started---------')\n",
    "    gs_results = gs.fit(X_train, y_train)\n",
    "\n",
    "    print('--------Done--------')\n",
    "    print('best score:')\n",
    "    print(gs_results.best_score_)\n",
    "    print('best estimator:')\n",
    "    print(gs_results.best_estimator_)\n",
    "    print('best params:')\n",
    "    print(gs_results.best_params_)\n",
    "    \n",
    "def gaussian_nb_fit_score(result):\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(result)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "    score = gnb.score(X_test, y_test)\n",
    "    print(score)\n",
    "    \n",
    "def searchHyperParams_svc(result):\n",
    "    print('--------start searching params--------')\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(result)\n",
    "    print('--------splitted train test----------')\n",
    "    grid_params = {\n",
    "        'C': [0.1,1, 10, 100],\n",
    "        'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': [1,0.1,0.01,0.001]\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        SVC(),\n",
    "        grid_params,\n",
    "        verbose = 1,\n",
    "        cv = 3,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    print('---------grid search started---------')\n",
    "    gs_results = gs.fit(X_train, y_train)\n",
    "\n",
    "    print('--------Done--------')\n",
    "    print('best score:')\n",
    "    print(gs_results.best_score_)\n",
    "    print('best estimator:')\n",
    "    print(gs_results.best_estimator_)\n",
    "    print('best params:')\n",
    "    print(gs_results.best_params_)\n",
    "    \n",
    "def searchHyperParams_DT(result):\n",
    "    print('--------start searching params--------')\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(result)\n",
    "    print('--------splitted train test----------')\n",
    "    grid_params = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'max_depth': [1,5,10,20,50,100,150,None],\n",
    "        'min_samples_split': np.arange(2,5),\n",
    "        'min_samples_leaf': np.arange(1,5),\n",
    "        'min_weight_fraction_leaf': np.arange(0,0.5)\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        DecisionTreeClassifier(),\n",
    "        grid_params,\n",
    "        verbose = 1,\n",
    "        cv = 3,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    print('---------grid search started---------')\n",
    "    gs_results = gs.fit(X_train, y_train)\n",
    "\n",
    "    print('--------Done--------')\n",
    "    print('best score:')\n",
    "    print(gs_results.best_score_)\n",
    "    print('best estimator:')\n",
    "    print(gs_results.best_estimator_)\n",
    "    print('best params:')\n",
    "    print(gs_results.best_params_)\n",
    "\n",
    "def searchHyperParams_RC(result):\n",
    "    print('--------start searching params--------')\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(result)\n",
    "    print('--------splitted train test----------')\n",
    "    grid_params = {\n",
    "        'bootstrap': [True, False],\n",
    "         'max_depth': [10, 20, 30, 40, None],\n",
    "         'max_features': ['auto', 'sqrt'],\n",
    "         'min_samples_leaf': [1, 2, 4],\n",
    "         'min_samples_split': [2, 5, 10],\n",
    "         'n_estimators': [200, 400, 600, 800]\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        RandomForestClassifier(),\n",
    "        grid_params,\n",
    "        verbose = 3,\n",
    "        cv = 3,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    print('---------grid search started---------')\n",
    "    gs_results = gs.fit(X_train, y_train)\n",
    "\n",
    "    print('--------Done--------')\n",
    "    print('best score:')\n",
    "    print(gs_results.best_score_)\n",
    "    print('best estimator:')\n",
    "    print(gs_results.best_estimator_)\n",
    "    print('best params:')\n",
    "    print(gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------start searching params--------\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "-----------dataset splitted------------\n",
      "--------splitted train test----------\n",
      "---------grid search started---------\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "--------Done--------\n",
      "best score:\n",
      "0.9421296296296297\n",
      "best estimator:\n",
      "KNeighborsClassifier(metric='manhattan', weights='distance')\n",
      "best params:\n",
      "{'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "searchHyperParams_knn(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "-----------dataset splitted------------\n",
      "0.8356481481481481\n"
     ]
    }
   ],
   "source": [
    "gaussian_nb_fit_score(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------start searching params--------\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "-----------dataset splitted------------\n",
      "--------splitted train test----------\n",
      "---------grid search started---------\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "--------Done--------\n",
      "best score:\n",
      "0.960648148148148\n",
      "best estimator:\n",
      "SVC(C=10, gamma=1)\n",
      "best params:\n",
      "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "searchHyperParams_svc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------start searching params--------\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "-----------dataset splitted------------\n",
      "--------splitted train test----------\n",
      "---------grid search started---------\n",
      "Fitting 3 folds for each of 384 candidates, totalling 1152 fits\n",
      "--------Done--------\n",
      "best score:\n",
      "0.83179012345679\n",
      "best estimator:\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=100, min_samples_split=3)\n",
      "best params:\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.0, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "searchHyperParams_DT(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchHyperParams_RC(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_fit_score(X_train, X_test, y_train, y_test):\n",
    "    train_samples = len(X_train)\n",
    "    test_samples = len (X_test)\n",
    "    neigh = SVC(C=10, gamma=1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    return score\n",
    "\n",
    "def kfolding_svc(dataset):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    global features\n",
    "    X_training = dataset[features]\n",
    "    y_training = dataset['zip_number']\n",
    "    k = 1\n",
    "    for train_index, test_index in kf.split(X_training, y_training):\n",
    "        \n",
    "        X_train, X_test = X_training.loc[train_index,:], X_training.loc[test_index,:]\n",
    "        y_train, y_test = y_training.loc[train_index], y_training.loc[test_index]\n",
    "        X_train, X_test = preprocess(X_train, X_test)\n",
    "        \n",
    "        score = svc_fit_score(X_train, X_test, y_train, y_test)\n",
    "        print(\"[fold {0}], score: {1:.5f}\".\n",
    "          format(k, score))\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "[fold 1], score: 0.96243\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "[fold 2], score: 0.97110\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "[fold 3], score: 0.96821\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "[fold 4], score: 0.97101\n",
      "-----Start preprocc----------\n",
      "---------Preprocess Done--------\n",
      "[fold 5], score: 0.95652\n"
     ]
    }
   ],
   "source": [
    "kfolding_svc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------score from test------------------- \n",
      "0.9595375722543352\n"
     ]
    }
   ],
   "source": [
    "all_zipcodes = []\n",
    "total_images = len(test_images_df.index)\n",
    "\n",
    "#standardScaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        result[features], result['zip_number'], test_size=0.2, random_state=0)\n",
    "\n",
    "X_train[features] = scaler.fit_transform(X_train[features].to_numpy())\n",
    "X_test[features] = scaler.transform(X_test[features].to_numpy())\n",
    "#train model\n",
    "svc_clf = SVC(C=10, gamma=1)\n",
    "svc_clf.fit(X_train, y_train)\n",
    "\n",
    "score = svc_clf.score(X_test, y_test)\n",
    "print('---------score from test------------------- ')\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, total_images):\n",
    "    \n",
    "    image_name = test_images_df.iloc[i, 0]\n",
    "    image = test_images_df.iloc[i, 1]\n",
    "    tmp_result_grid = initialize_image_grids(image, image_name)\n",
    "    tmp_result_props = initialize_image_props(image, image_name)\n",
    "    test_result = pd.concat([tmp_result_grid,tmp_result_props], axis=1)\n",
    "    test_result.reset_index(inplace=True,drop=True)\n",
    "    test_result['zip_number'] = test_result['zip_number'].astype(int)\n",
    "    test_result[features] = scaler.transform(test_result[features].to_numpy())\n",
    "    \n",
    "    # classify\n",
    "    predicted = svc_clf.predict(test_result[features])\n",
    "    #print(predicted)\n",
    "    all_zipcodes.append(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  1 4 8 9\n",
      "original:  ['1489']\n",
      "predicted:  1 8 6 5\n",
      "original:  ['1865']\n",
      "predicted:  1 9 0 4\n",
      "original:  ['1904']\n",
      "predicted:  2 6 0 8\n",
      "original:  ['2608']\n",
      "predicted:  2 7 0 8\n",
      "original:  ['2708']\n",
      "predicted:  2 9 7 1\n",
      "original:  ['2971']\n",
      "predicted:  3 3 4 6\n",
      "original:  ['3046']\n",
      "predicted:  3 4 1 7\n",
      "original:  ['3417']\n",
      "predicted:  5 5 2 1\n",
      "original:  ['3521']\n",
      "predicted:  3 5 3 6\n",
      "original:  ['3536']\n",
      "predicted:  3 5 9 1\n",
      "original:  ['3591']\n",
      "predicted:  3 6 0 6\n",
      "original:  ['3606']\n",
      "predicted:  3 6 4 1\n",
      "original:  ['3648']\n",
      "predicted:  3 7 1 1\n",
      "original:  ['3711']\n",
      "predicted:  3 7 4 6\n",
      "original:  ['3746']\n",
      "predicted:  3 9 2 1\n",
      "original:  ['3921']\n",
      "predicted:  3 9 8 9\n",
      "original:  ['3988']\n",
      "predicted:  4 0 0 6\n",
      "original:  ['4006']\n",
      "predicted:  4 1 3 4\n",
      "original:  ['4134']\n",
      "predicted:  4 7 6 9\n",
      "original:  ['4764']\n",
      "predicted:  5 0 7 1\n",
      "original:  ['5071']\n",
      "predicted:  5 5 3 9\n",
      "original:  ['5539']\n",
      "predicted:  5 5 5 1\n",
      "original:  ['5551']\n",
      "predicted:  5 5 7 8\n",
      "original:  ['5578']\n",
      "predicted:  5 6 4 3\n",
      "original:  ['5643']\n",
      "predicted:  6 3 9 6\n",
      "original:  ['6396']\n",
      "predicted:  6 3 3 9\n",
      "original:  ['6399']\n",
      "predicted:  6 4 0 3\n",
      "original:  ['6403']\n",
      "predicted:  6 6 3 9\n",
      "original:  ['6639']\n",
      "predicted:  6 6 5 2\n",
      "original:  ['6652']\n",
      "predicted:  6 6 5 8\n",
      "original:  ['6658']\n",
      "predicted:  7 1 6 9\n",
      "original:  ['7169']\n",
      "predicted:  7 2 5 6\n",
      "original:  ['7256']\n",
      "predicted:  7 7 7 9\n",
      "original:  ['7779']\n",
      "predicted:  8 0 6 4\n",
      "original:  ['8060']\n",
      "predicted:  8 3 9 5\n",
      "original:  ['8395']\n",
      "predicted:  8 4 3 2\n",
      "original:  ['8432']\n",
      "predicted:  8 5 4 3\n",
      "original:  ['8543']\n",
      "predicted:  8 8 9 3\n",
      "original:  ['8893']\n",
      "predicted:  8 9 9 7\n",
      "original:  ['8997']\n",
      "predicted:  9 0 0 2\n",
      "original:  ['9002']\n",
      "predicted:  3 1 1 1\n",
      "original:  ['9111']\n",
      "predicted:  9 3 0 6\n",
      "original:  ['9306']\n",
      "predicted:  9 3 1 0\n",
      "original:  ['9310']\n",
      "predicted:  9 5 3 4\n",
      "original:  ['9534']\n",
      "predicted:  9 6 1 1\n",
      "original:  ['9611']\n",
      "predicted:  9 7 2 6\n",
      "original:  ['9726']\n",
      "predicted:  9 8 3 8\n",
      "original:  ['9838']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(all_zipcodes)):\n",
    "    print('predicted: ', all_zipcodes[i][0], all_zipcodes[i][1], all_zipcodes[i][2], all_zipcodes[i][3])\n",
    "    print('original: ',  test_images_df.loc[i,['name']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
